{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0124b0db",
   "metadata": {},
   "source": [
    "# Student self-graded homework workflow\n",
    "Author: Dr. Matthew Ford (mattford@uw.edu) [Website](https://dashdotrobot.com)\n",
    "\n",
    "M. Ford and H. E. Dillon, __[A secure, scalable approach to student-graded homework for self-reflection](https://peer.asee.org/a-secure-scalable-approach-to-student-graded-homework-for-self-reflection)__. 2024 ASEE Annual Conference & Exposition, Portland, Oregon. 10.18260/1-2--46489\n",
    "\n",
    "Use this script to compare student self-graded scores to instructor scores and enter final grades into the Canvas gradebook.\n",
    "\n",
    "### Workflow for a student self-graded homework assignment\n",
    "\n",
    "1. __Assign homework on Canvas__: Create a zero-point assignment called \"Homework X - submit to Gradescope\". This assignment should have the problem set, instructions, etc.\n",
    "2. __Create Gradescope assignment__: Create a Gradescope assignment with the correct number of problems. Each problem should have a point value of 0. Students will actually submit their assignment here.\n",
    "3. __Create a solution and grading rubric__: The grading rubric should be clear enough for students to follow. I make my rubric into a Word Doc that students fill out when they grade their work. Upload the solution and rubric to Canvas, but set them to be unavailable until after the homework deadline.\n",
    "5. __Create a Canvas Quiz for self-grading__: Create a Canvas Quiz (\"Classic Quiz\" or \"Legacy Quiz\") called \"Homework X - enter your own grade\". I put a Text Only question at the top of the quiz with links to the solution and rubric. The quiz should have a Numeric question for each homework problem. The name of each problem should be \"Problem Y\" with the problem number and the text should be \"Please enter your score for Problem Y.\" The point value for the question should match the point value for the homework problem. Since the quiz score itself is irrelevant, you can make a single correct answer \"-1\".\n",
    "6. __Set the self-grading quiz \"Available From\" time__: Set the quiz to become available just after the homework deadline. I suggest a 2 hour gap or so to account for late submissions.\n",
    "\n",
    "After students submit their homework, the self-grading assignment becomes available and students are able to review the solutions and enter their scores into the Canvas quiz. The instructor or TAs should pick a \"check problem\" to grade via Gradescope and compare to students' self-assessments.\n",
    "\n",
    "7. __Grade check problem on Gradescope__: Change the point value of the check problem on Gradescope to match the actual points and create a rubric. After you're done grading, sync the Gradescope score back to the Canvas assignment \"Homework X - submit to Gradescope\".\n",
    "8. __Reconcile and sync scores__: After you and the students have submitted your grades, run this script to compare scores. This script will generate a log file which you can check for consistency. You may want to look more carefully at submissions with a large discrepancy between the instructor score and student score. Finally, change `enter_into_gradebook` to `True` and run this script again to publish final scores to Canvas. You may have to open Canvas to publish the assignment \"Homework X - final grade\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f3bf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from canvasapi import Canvas\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from numpy import argmax\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b50faa6",
   "metadata": {},
   "source": [
    "## Canvas course settings\n",
    "\n",
    "Enter the number after https://canvas.[institution].edu/courses/ in your course URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8e3218",
   "metadata": {},
   "outputs": [],
   "source": [
    "course_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b1fc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('API_KEY.txt') as f:\n",
    "    API_KEY = f.readline().strip()\n",
    "\n",
    "canvas = Canvas('https://canvas.uw.edu', API_KEY)\n",
    "course = canvas.get_course(course_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15d0477",
   "metadata": {},
   "source": [
    "## Assignment settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6adb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment name\n",
    "assignment_name = \"Homework 1\"\n",
    "\n",
    "# Name of the problem to be checked against the instructor assigned score\n",
    "check_problem = 'Problem 1'\n",
    "\n",
    "# Name of self-grading Canvas quiz\n",
    "self_grade_name = assignment_name + \" - enter your own grade\"\n",
    "\n",
    "# Name of Canvas assignment for syncing Gradescope scores for one problem\n",
    "inst_grade_name = assignment_name + \" - submit to Gradescope\"\n",
    "\n",
    "# Name of Canvas assignment to store final assignment score (the script will create this automaticaly)\n",
    "final_grade_name = assignment_name + \" - final grade\"\n",
    "\n",
    "# Filename for the status log file\n",
    "log_filename = assignment_name + \"_log.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46849dc0",
   "metadata": {},
   "source": [
    "## Update settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0b7df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "enter_into_gradebook = True   # Set to False to generate a report without entering grades\n",
    "replace_student_score = True  # Set to False to default to student scores for check problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840e9590",
   "metadata": {},
   "source": [
    "## Reconcile grades\n",
    "\n",
    "Compare student score to instructor score for check problem and generate a log file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f3cb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch self-grading assignment\n",
    "a_self = course.get_assignments(search_term=self_grade_name)[0]\n",
    "a_inst = course.get_assignments(search_term=inst_grade_name)[0]\n",
    "\n",
    "# Get point values of quiz questions corresponding to Problems\n",
    "q_probs = course.get_quiz(a_self.quiz_id).get_questions()\n",
    "q_point_vals = {q.id:q.points_possible for q in q_probs if 'Problem' in q.question_name}\n",
    "q_names = {q.id:q.question_name for q in q_probs if 'Problem' in q.question_name}\n",
    "\n",
    "# Calculate total assignment point value\n",
    "total_point_val = sum(q_point_vals.values())\n",
    "\n",
    "# Fetch or create final grade assignment\n",
    "try:\n",
    "    a_final = course.get_assignments(search_term=final_grade_name)[0]\n",
    "    print(final_grade_name, 'already exists.')\n",
    "except IndexError as e:\n",
    "    print(final_grade_name, 'does not exist. Creating new assignment...')\n",
    "    a_final = course.create_assignment({\n",
    "        'name': final_grade_name,\n",
    "        'points_possible': total_point_val\n",
    "    })\n",
    "\n",
    "# Log file columns:\n",
    "cols = ['user_id', 'name', 'total_score', 'self_score', 'inst_score', 'diff', 'notes']\n",
    "\n",
    "with open(log_filename, 'w', newline='') as log_file:\n",
    "    log = csv.DictWriter(log_file, fieldnames=cols)\n",
    "    log.writeheader()\n",
    "\n",
    "    for user in course.get_users(enrollment_type=['student']):\n",
    "\n",
    "        try:\n",
    "\n",
    "            # Get self grade submission history (includes all attempts)\n",
    "            s = a_self.get_submission(user=user.id, include='submission_history')\n",
    "\n",
    "            # Get latest submission\n",
    "            sh_index = argmax([sh['attempt'] for sh in s.submission_history])\n",
    "            sh_latest = s.submission_history[sh_index]\n",
    "\n",
    "            if 'submission_data' not in sh_latest:\n",
    "                raise KeyError('No student self grade found')\n",
    "\n",
    "            # Get instructor score\n",
    "            x = a_inst.get_submission(user=user.id)\n",
    "\n",
    "            if x.grade is None:\n",
    "                raise KeyError('No instructor grade found for student')\n",
    "\n",
    "            inst_check_score = float(x.grade)\n",
    "            self_check_score = 0\n",
    "            total_score = 0\n",
    "\n",
    "            for q in sh_latest['submission_data']:    # Loop over quiz questions\n",
    "                if q['question_id'] in q_point_vals:  # If Problem\n",
    "                    student_score = float(q['text'])\n",
    "                    \n",
    "                    if student_score > q_point_vals[q['question_id']]:\n",
    "                        raise ValueError('Student entered score greater than maximum.')\n",
    "                        \n",
    "                    if student_score < 0:\n",
    "                        raise ValueError('Student entered a score less than zero.')\n",
    "\n",
    "                    if q_names[q['question_id']] == check_problem:\n",
    "                        self_check_score = student_score\n",
    "                        if replace_student_score:\n",
    "                            total_score += inst_check_score\n",
    "                        else:\n",
    "                            total_score += student_score\n",
    "                    else:\n",
    "                        total_score += student_score\n",
    "\n",
    "            # Update score in Canvas\n",
    "            if enter_into_gradebook:\n",
    "                s_final = a_final.get_submission(user.id)\n",
    "                s_final.edit(submission={'posted_grade':total_score})\n",
    "                        \n",
    "            # Log score info\n",
    "            log.writerow({'user_id': user.id,\n",
    "                          'name': user.name,\n",
    "                          'total_score': total_score,\n",
    "                          'self_score': self_check_score,\n",
    "                          'inst_score': inst_check_score,\n",
    "                          'diff': inst_check_score - self_check_score})\n",
    "\n",
    "        except Exception as e:  # An error occurred\n",
    "            print(repr(e))\n",
    "            log.writerow({'user_id': user.id, 'name': user.name, 'notes': repr(e)})\n",
    "            \n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fc2cb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "canvas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
